{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats as st\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_state = np.random.RandomState(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently working directory\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# load functions from other notebooks\n",
    "helpers_file = os.path.join(base_dir, 'helpers.ipynb')\n",
    "%run $helpers_file\n",
    "\n",
    "# Load spotlight module\n",
    "for p in ['../spotlight_ext']:\n",
    "    module_path = os.path.abspath(os.path.join(base_dir, p))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model(model_type='entire')\n",
    "pooling_model = load_model('pooling')\n",
    "\n",
    "pretrained_models = {\n",
    "    'lstm': lstm_model,\n",
    "    'pooling': pooling_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "\n",
    "# get dataset\n",
    "dataset = get_movielens_dataset(variant='1M')\n",
    "train, test = random_train_test_split(dataset, random_state=random_state)\n",
    "\n",
    "max_sequence_length = 20\n",
    "train = train.to_sequence(max_sequence_length=max_sequence_length)\n",
    "test = test.to_sequence(max_sequence_length=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target item is 149 in this test case, top k is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59, 114, 124, 125, 177, 186, 190, 191, 196, 197, 200], dtype=int32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interaction = test.sequences[test.user_ids == 3][0].copy()\n",
    "test_interaction = test_interaction[test_interaction != 0]\n",
    "test_interaction.sort()\n",
    "test_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_test_interaction = len(test_interaction)\n",
    "len_test_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_item(model, test_interaction, position=1):\n",
    "    prediction = model.predict(test_interaction)\n",
    "    prediction[test_interaction] = -StaticVars.FLOAT_MAX\n",
    "    rk_data = st.rankdata(-prediction, method='ordinal')\n",
    "    index = np.where(rk_data == position)\n",
    "    return index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_position_item(pooling_model, test_interaction, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random CF candidate selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_sublists(original_list, sublists_info):\n",
    "    result_sublists = []\n",
    "    rng = np.random.default_rng(seed=2020)  # Seed for reproducibility\n",
    "\n",
    "    for length, count in sublists_info.items():\n",
    "        generated_sublists_for_length = set()\n",
    "\n",
    "        while len(generated_sublists_for_length) < count:\n",
    "            sublist = tuple(rng.choice(original_list, length, replace=False))\n",
    "            generated_sublists_for_length.add(sublist)\n",
    "\n",
    "        result_sublists.extend(np.array(list(sublist)) for sublist in generated_sublists_for_length)\n",
    "\n",
    "    return result_sublists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossover and Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def crossover(first_list, second_list, p_cross):\n",
    "    # Find the shorter length among the two lists\n",
    "    length_first = len(first_list)\n",
    "    length_second = len(second_list)\n",
    "    shorter_length = min(length_first, length_second)\n",
    "    \n",
    "    # Compute the number of crossover points\n",
    "    num_crossovers = int(shorter_length * p_cross)\n",
    "    \n",
    "    # Choose random indices for crossover within the range of shorter length\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    crossover_indices_first = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    crossover_indices_second = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    \n",
    "    # Sort the crossover indices\n",
    "    crossover_indices_first.sort()\n",
    "    crossover_indices_second.sort()\n",
    "    \n",
    "    # Swap the elements at the crossover indices\n",
    "    for i in range(num_crossovers):\n",
    "        index_first = crossover_indices_first[i]\n",
    "        index_second = crossover_indices_second[i]\n",
    "        first_list[index_first], second_list[index_second] = second_list[index_second], first_list[index_first]\n",
    "    \n",
    "    return first_list, second_list\n",
    "\n",
    "def mutate_array(org_arr, arr_to_mutate, mutation_probability):\n",
    "    # Calculate the number of elements to mutate\n",
    "    num_mutations = int(mutation_probability * len(arr_to_mutate))\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    # Select the indices to mutate\n",
    "    indices_to_mutate = rng.choice(range(len(arr_to_mutate)), size=num_mutations, replace=False)\n",
    "    \n",
    "    # Mutate the selected elements\n",
    "    for idx in indices_to_mutate:\n",
    "        arr_to_mutate[idx] = rng.choice(org_arr)\n",
    "\n",
    "    return arr_to_mutate\n",
    "\n",
    "\n",
    "def remove_duplicates(arr):\n",
    "    _, idx = np.unique(arr, return_index=True)\n",
    "    return arr[np.sort(idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "class StaticVars:\n",
    "    FLOAT_MAX = float('inf')\n",
    "\n",
    "def supersets_of_new_subsets_of_old(new_cf, old_cf):\n",
    "    diff = np.setdiff1d(old_cf, new_cf)  # Elements that are in old_cf but not in new_cf\n",
    "    for r in range(1, len(diff) + 1):\n",
    "        for subset in combinations(diff, r):\n",
    "            yield np.union1d(new_cf, subset)\n",
    "\n",
    "def compute_yloss(target_score, kth_score):\n",
    "    yloss = max(0, target_score / kth_score - 1.0)\n",
    "    return yloss\n",
    "\n",
    "def compute_distance(x, y):\n",
    "    diff = np.setdiff1d(x, y)\n",
    "    return len(diff)\n",
    "\n",
    "def compute_loss(old_cf, new_cf, model, target_item, top_k, yloss_cache):\n",
    "    cache_key = frozenset(new_cf)\n",
    "    if cache_key in yloss_cache:\n",
    "        yloss = yloss_cache[cache_key]\n",
    "    else:\n",
    "        new_prediction = model.predict(new_cf)\n",
    "        new_prediction[new_cf] = -StaticVars.FLOAT_MAX\n",
    "        new_rk_data = st.rankdata(-new_prediction, method='ordinal')\n",
    "\n",
    "        top_k_index = np.where(new_rk_data == top_k)[0][0]\n",
    "        yloss = compute_yloss(new_prediction[target_item], new_prediction[top_k_index])\n",
    "        yloss_cache[cache_key] = yloss\n",
    "    dis = compute_distance(old_cf, new_cf)\n",
    "\n",
    "    subset_yloss = 0\n",
    "    for superset in supersets_of_new_subsets_of_old(new_cf, old_cf):\n",
    "        cache_key = frozenset(superset)\n",
    "        if cache_key in yloss_cache:\n",
    "            subset_yloss += yloss_cache[cache_key]\n",
    "        else:\n",
    "            subset_prediction = model.predict(superset)\n",
    "            subset_prediction[superset] = -StaticVars.FLOAT_MAX\n",
    "            sub_rk_data = st.rankdata(-subset_prediction, method='ordinal')\n",
    "            sub_top_k_index = np.where(sub_rk_data == top_k)[0][0]\n",
    "            subset_yloss += compute_yloss(subset_prediction[target_item], subset_prediction[sub_top_k_index])\n",
    "            yloss_cache[cache_key] = subset_yloss\n",
    "\n",
    "    return list([yloss, dis, subset_yloss])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSGA-II\n",
    "Apply NSGA-II to the problem of finding the optimal candicates in multi-objective optimization problem.\n",
    "Based on:\n",
    "- Non-domination Rank\n",
    "- Crowding Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominates(row, candidateRow):\n",
    "    \"\"\"Determine if one solution dominates another\"\"\"\n",
    "    return all(r <= cr for r, cr in zip(row, candidateRow)) and any(r < cr for r, cr in zip(row, candidateRow))\n",
    "\n",
    "def crowding_distance_assignment(front, values):\n",
    "    distances = [0] * len(values)  # Initialize the distance for every solution as 0\n",
    "    num_objs = len(values[0])\n",
    "    \n",
    "    for m in range(num_objs):\n",
    "        sorted_front = sorted(front, key=lambda x: values[x][m])\n",
    "\n",
    "        # Assign infinite distance at boundaries.\n",
    "        distances[sorted_front[0]] = distances[sorted_front[-1]] = float('inf')\n",
    "\n",
    "        # Normalize the objective values for distance computation.\n",
    "        obj_min = values[sorted_front[0]][m]\n",
    "        obj_max = values[sorted_front[-1]][m]\n",
    "        denom = obj_max - obj_min if obj_max != obj_min else 1\n",
    "\n",
    "        for i in range(1, len(sorted_front) - 1):\n",
    "            distances[sorted_front[i]] += (values[sorted_front[i + 1]][m] - values[sorted_front[i - 1]][m]) / denom\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "def fast_nondominated_sort(values):\n",
    "    \"\"\"NSGA-II's fast non-dominated sort\"\"\"\n",
    "    S = [[] for _ in range(len(values))]\n",
    "    front = [[]]\n",
    "    n = [0 for _ in range(len(values))]\n",
    "    rank = [-1 for _ in range(len(values))]\n",
    "    \n",
    "    for p in range(len(values)):\n",
    "        S[p] = []\n",
    "        n[p] = 0\n",
    "        for q in range(len(values)):\n",
    "            if dominates(values[p], values[q]):\n",
    "                S[p].append(q)\n",
    "            elif dominates(values[q], values[p]):\n",
    "                n[p] += 1\n",
    "        if n[p] == 0:\n",
    "            rank[p] = 0\n",
    "            front[0].append(p)\n",
    "            \n",
    "    i = 0\n",
    "    while front[i]:\n",
    "        nextFront = []\n",
    "        for p in front[i]:\n",
    "            for q in S[p]:\n",
    "                n[q] = n[q] - 1\n",
    "                if n[q] == 0:\n",
    "                    rank[q] = i + 1\n",
    "                    nextFront.append(q)\n",
    "        i = i + 1\n",
    "        front.append(nextFront)\n",
    "\n",
    "    del front[len(front) - 1]\n",
    "    \n",
    "    # Initialize crowding distances as zeros\n",
    "    crowding_distances = [0] * len(values)\n",
    "    \n",
    "    for front_solutions in front:\n",
    "        current_front_distances = crowding_distance_assignment(front_solutions, values)\n",
    "        for j, solution in enumerate(front_solutions):\n",
    "            crowding_distances[solution] = current_front_distances[solution]\n",
    "    \n",
    "    return rank, crowding_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_pairs(list_of_arrays, n):\n",
    "    # Generate all possible pairs\n",
    "    random.seed(2020)\n",
    "    all_pairs = list(itertools.combinations(list_of_arrays, 2))\n",
    "\n",
    "    # Randomly select n pairs\n",
    "    random_pairs = random.sample(all_pairs, n)\n",
    "\n",
    "    return random_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(interaction, candidates, model, target, k, yloss_cache, crossover_p, mutation_p, budget):\n",
    "    print(len(candidates))\n",
    "    pairs = generate_random_pairs(candidates, len(candidates)//2)\n",
    "    for first, second in pairs:\n",
    "        budget -= 1\n",
    "        first, second = crossover(first, second, crossover_p)\n",
    "        first = mutate_array(interaction, first, mutation_p)\n",
    "        second = mutate_array(interaction, second, mutation_p)\n",
    "        first = remove_duplicates(first)\n",
    "        second = remove_duplicates(second)\n",
    "        candidates.append(first)\n",
    "        candidates.append(second)\n",
    "    print(len(candidates))\n",
    "    losses = [compute_loss(interaction, arr, model, target, k, yloss_cache) for arr in candidates]\n",
    "    budget -= len(candidates)\n",
    "    print(losses)\n",
    "    solved = False\n",
    "    solved_list = []\n",
    "    for i in range(len(losses)):\n",
    "        if losses[i][0] == 0:\n",
    "            solved = True\n",
    "            solved_list.append(candidates[i])\n",
    "    if solved:\n",
    "        return solved_list, solved, budget\n",
    "    ranks, crowding_distances = fast_nondominated_sort(losses)\n",
    "    print(ranks)\n",
    "    candidates_with_metrics = list(zip(candidates, ranks, crowding_distances))\n",
    "\n",
    "    # Sort based on ranks (ascending) and then crowding distances (descending)\n",
    "    candidates_with_metrics.sort(key=lambda x: (x[1], -x[2]))\n",
    "\n",
    "    # Extract candidates after sorting\n",
    "    sorted_candidates = [pair[0] for pair in candidates_with_metrics]\n",
    "\n",
    "    # Extract the top third of candidates\n",
    "    least_loss_arrays = sorted_candidates[:len(sorted_candidates)//2]\n",
    "\n",
    "    return least_loss_arrays, solved, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, test_interaction, rank, sublists_info, top_k, crossover_p, mutation_p, budget):\n",
    "    target = get_position_item(model, test_interaction, rank)\n",
    "    new_gen = generate_random_sublists(test_interaction, sublists_info)\n",
    "    budget -= 1\n",
    "    solved = False\n",
    "    yloss_cache = {}\n",
    "    while solved is not True:\n",
    "        new_gen, solved, budget = generation(test_interaction, new_gen, model, target, top_k, yloss_cache, crossover_p, mutation_p, budget)\n",
    "        if budget == 0:\n",
    "            break\n",
    "    return new_gen, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "29\n",
      "[[0.15775024890899658, 3, 1.0352857112884521], [0.09447741508483887, 4, 3.743652820587158], [0, 3, 2.246750593185425], [0.13993871212005615, 3, 2.6697992086410522], [0, 5, 9.13612186908722], [0.06763672828674316, 3, 6.433834075927734], [0.03534376621246338, 4, 6.556341886520386], [0.11601829528808594, 3, 4.803943395614624], [0.03865993022918701, 3, 3.9157055616378784], [0.13430464267730713, 2, 4.7797956466674805], [0.020376086235046387, 4, 10.445356130599976], [0.13138890266418457, 4, 7.883714199066162], [0.5975372791290283, 1, 1.0352857112884521], [0.6852766275405884, 1, 1.0352857112884521], [0.20534729957580566, 4, 16.949491500854492], [0.11601829528808594, 3, 5.509027481079102], [0.07860791683197021, 3, 10.648540377616882], [0.09447741508483887, 4, 13.043522000312805], [0.07860791683197021, 3, 10.648540377616882], [0.03865993022918701, 3, 8.407523155212402], [0.020376086235046387, 4, 15.820550203323364], [0.020376086235046387, 4, 15.820550203323364], [0.2229156494140625, 5, 34.63867390155792], [0, 4, 14.086969137191772], [0.20534729957580566, 4, 24.044049382209778], [0, 5, 19.526177763938904], [0.13138890266418457, 4, 20.04058527946472], [0.06763672828674316, 3, 7.554722785949707], [0.03534376621246338, 4, 8.089012742042542]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([114, 190, 177, 125, 124, 191, 200,  59], dtype=int32),\n",
       "  array([186, 200, 124, 186, 186, 114, 177, 125], dtype=int32),\n",
       "  array([186, 200, 124, 191, 114, 177, 125], dtype=int32),\n",
       "  array([186, 200, 124, 114, 177, 125], dtype=int32)],\n",
       " 963)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yloss_cache = {}\n",
    "sublists_info ={\n",
    "    8:5,\n",
    "    9:5,\n",
    "    10:5\n",
    "}\n",
    "# target = get_position_item(pooling_model, test_interaction, 1)\n",
    "# first_gen = generate_random_sublists(test_interaction, sublists_info)\n",
    "# generation(test_interaction, first_gen, lstm_model, target, 10, yloss_cache, 0.3, 0.2)\n",
    "main(pooling_model, test_interaction, 1, sublists_info, 10, 0.3, 0.2, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
