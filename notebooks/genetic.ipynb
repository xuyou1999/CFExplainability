{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats as st\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_state = np.random.RandomState(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently working directory\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# load functions from other notebooks\n",
    "helpers_file = os.path.join(base_dir, 'helpers.ipynb')\n",
    "%run $helpers_file\n",
    "\n",
    "# Load spotlight module\n",
    "for p in ['../spotlight_ext']:\n",
    "    module_path = os.path.abspath(os.path.join(base_dir, p))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defined two pre-trained recommendation model to be explained in this project: lstm and pooling. Because lstm also concerns the order of the initial interaction list, it is not suitable for the genetic algorithm method. Therefore, only pooling model is used for testing and bechmarking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model(model_type='entire')\n",
    "pooling_model = load_model('pooling')\n",
    "\n",
    "pretrained_models = {\n",
    "    'lstm': lstm_model,\n",
    "    'pooling': pooling_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "\n",
    "# get dataset\n",
    "dataset = get_movielens_dataset(variant='1M')\n",
    "train, test = random_train_test_split(dataset, random_state=random_state)\n",
    "\n",
    "max_sequence_length = 20\n",
    "train = train.to_sequence(max_sequence_length=max_sequence_length)\n",
    "test = test.to_sequence(max_sequence_length=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59, 114, 124, 125, 177, 186, 190, 191, 196, 197, 200], dtype=int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interaction = test.sequences[test.user_ids == 3][0].copy()\n",
    "test_interaction = test_interaction[test_interaction != 0]\n",
    "test_interaction.sort()\n",
    "test_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_test_interaction = len(test_interaction)\n",
    "len_test_interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a specific instance, this function takes a position and gives the item id in that position of the predicted item list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_item(model, test_interaction, position=1):\n",
    "    prediction = model.predict(test_interaction)\n",
    "    prediction[test_interaction] = -StaticVars.FLOAT_MAX\n",
    "    rk_data = st.rankdata(-prediction, method='ordinal')\n",
    "    index = np.where(rk_data == position)\n",
    "    return index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_position_item(pooling_model, test_interaction, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random CF candidate selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function randomly generates a set of subsets from a specific array. The number of array to be generated is defined by \"sublists_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_sublists(original_list, sublists_info):\n",
    "    result_sublists = []\n",
    "    rng = np.random.default_rng(seed=2020)  # Seed for reproducibility\n",
    "\n",
    "    for length, count in sublists_info.items():\n",
    "        generated_sublists_for_length = set()\n",
    "\n",
    "        while len(generated_sublists_for_length) < count:\n",
    "            sublist = tuple(rng.choice(original_list, length, replace=False))\n",
    "            generated_sublists_for_length.add(sublist)\n",
    "\n",
    "        result_sublists.extend(np.array(list(sublist)) for sublist in generated_sublists_for_length)\n",
    "\n",
    "    return result_sublists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossover and Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions perform crossover and mutation job. The crossover function takes two parents and returns two children. The mutation function takes a parent and returns a child. The number of elements to crossover and mutate is defiend by the probablity parameter. For instance, 0.3 crossover prob means that 30% of the elements will be swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def crossover(first_list, second_list, p_cross):\n",
    "    # Find the shorter length among the two lists\n",
    "    length_first = len(first_list)\n",
    "    length_second = len(second_list)\n",
    "    shorter_length = min(length_first, length_second)\n",
    "    \n",
    "    # Compute the number of crossover points\n",
    "    num_crossovers = max(int(shorter_length * p_cross), 1)\n",
    "    \n",
    "    # Choose random indices for crossover\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    crossover_indices_first = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    crossover_indices_second = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    \n",
    "    # Sort the crossover indices\n",
    "    crossover_indices_first.sort()\n",
    "    crossover_indices_second.sort()\n",
    "    \n",
    "    # Swap the elements at the crossover indices\n",
    "    for i in range(num_crossovers):\n",
    "        index_first = crossover_indices_first[i]\n",
    "        index_second = crossover_indices_second[i]\n",
    "        first_list[index_first], second_list[index_second] = second_list[index_second], first_list[index_first]\n",
    "    \n",
    "    return first_list, second_list\n",
    "\n",
    "def mutate_array(org_arr, arr_to_mutate, mutation_probability):\n",
    "    # Calculate the number of elements to mutate\n",
    "    num_mutations = max(int(mutation_probability * len(arr_to_mutate)), 1)\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    # Select the indices to mutate\n",
    "    indices_to_mutate = rng.choice(range(len(arr_to_mutate)), size=num_mutations, replace=False)\n",
    "    \n",
    "    # Mutate the selected elements\n",
    "    for idx in indices_to_mutate:\n",
    "        arr_to_mutate[idx] = rng.choice(org_arr)\n",
    "\n",
    "    return arr_to_mutate\n",
    "\n",
    "\n",
    "def remove_duplicates(arr):\n",
    "    _, idx = np.unique(arr, return_index=True)\n",
    "    return arr[np.sort(idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three loss functions defined: y-loss, distance, and y-loss for subsets of removing items (supersets of candidate CFs).\n",
    "- y-loss: How far is the target item away from the k-th item, concerning their prediction scores.\n",
    "- distance: How many elements are removed from the interaction list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "class StaticVars:\n",
    "    FLOAT_MAX = float('inf')\n",
    "\n",
    "def supersets_of_new_subsets_of_old(new_cf, old_cf):\n",
    "    diff = np.setdiff1d(old_cf, new_cf)  # Elements that are in old_cf but not in new_cf\n",
    "    for r in range(1, len(diff) + 1):\n",
    "        for subset in combinations(diff, r):\n",
    "            yield np.union1d(new_cf, subset)\n",
    "\n",
    "def compute_yloss(target_score, kth_score):\n",
    "    yloss = max(0, target_score / kth_score - 1.0)\n",
    "    return yloss\n",
    "\n",
    "def compute_distance(x, y):\n",
    "    diff = np.setdiff1d(x, y)\n",
    "    return len(diff)\n",
    "\n",
    "def compute_loss(old_cf, new_cf, model, target_item, top_k, yloss_cache):\n",
    "    cache_key = frozenset(new_cf)\n",
    "    if cache_key in yloss_cache:\n",
    "        yloss = yloss_cache[cache_key]\n",
    "    else:\n",
    "        new_prediction = model.predict(new_cf)\n",
    "        new_prediction[new_cf] = -StaticVars.FLOAT_MAX\n",
    "        new_rk_data = st.rankdata(-new_prediction, method='ordinal')\n",
    "\n",
    "        top_k_index = np.where(new_rk_data == top_k)[0][0]\n",
    "        yloss = compute_yloss(new_prediction[target_item], new_prediction[top_k_index])\n",
    "        yloss_cache[cache_key] = yloss\n",
    "    dis = compute_distance(old_cf, new_cf)\n",
    "\n",
    "    subset_yloss = 0\n",
    "    for superset in supersets_of_new_subsets_of_old(new_cf, old_cf):\n",
    "        cache_key = frozenset(superset)\n",
    "        if cache_key in yloss_cache:\n",
    "            # print(\"Cache hit\")\n",
    "            subset_yloss += yloss_cache[cache_key]\n",
    "        else:\n",
    "            subset_prediction = model.predict(superset)\n",
    "            subset_prediction[superset] = -StaticVars.FLOAT_MAX\n",
    "            sub_rk_data = st.rankdata(-subset_prediction, method='ordinal')\n",
    "            sub_top_k_index = np.where(sub_rk_data == top_k)[0][0]\n",
    "            subset_yloss += compute_yloss(subset_prediction[target_item], subset_prediction[sub_top_k_index])\n",
    "            yloss_cache[cache_key] = subset_yloss\n",
    "\n",
    "    return list([yloss, dis, subset_yloss])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSGA-II\n",
    "Apply NSGA-II to the problem of finding the optimal candicates in multi-objective optimization problem. It gives ranks to all candidate CFs based on:\n",
    "- Non-domination Rank\n",
    "- Crowding Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominates(row, candidateRow):\n",
    "    \"\"\"Determine if one solution dominates another\"\"\"\n",
    "    return all(r <= cr for r, cr in zip(row, candidateRow)) and any(r < cr for r, cr in zip(row, candidateRow))\n",
    "\n",
    "def crowding_distance_assignment(front, values):\n",
    "    distances = [0] * len(values)  # Initialize the distance for every solution as 0\n",
    "    num_objs = len(values[0])\n",
    "    \n",
    "    for m in range(num_objs):\n",
    "        sorted_front = sorted(front, key=lambda x: values[x][m])\n",
    "\n",
    "        # Assign infinite distance at boundaries.\n",
    "        distances[sorted_front[0]] = distances[sorted_front[-1]] = float('inf')\n",
    "\n",
    "        # Normalize the objective values for distance computation.\n",
    "        obj_min = values[sorted_front[0]][m]\n",
    "        obj_max = values[sorted_front[-1]][m]\n",
    "        denom = obj_max - obj_min if obj_max != obj_min else 1\n",
    "\n",
    "        for i in range(1, len(sorted_front) - 1):\n",
    "            distances[sorted_front[i]] += (values[sorted_front[i + 1]][m] - values[sorted_front[i - 1]][m]) / denom\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "def fast_nondominated_sort(values):\n",
    "    \"\"\"NSGA-II's fast non-dominated sort\"\"\"\n",
    "    S = [[] for _ in range(len(values))]\n",
    "    front = [[]]\n",
    "    n = [0 for _ in range(len(values))]\n",
    "    rank = [-1 for _ in range(len(values))]\n",
    "    \n",
    "    for p in range(len(values)):\n",
    "        S[p] = []\n",
    "        n[p] = 0\n",
    "        for q in range(len(values)):\n",
    "            if dominates(values[p], values[q]):\n",
    "                S[p].append(q)\n",
    "            elif dominates(values[q], values[p]):\n",
    "                n[p] += 1\n",
    "        if n[p] == 0:\n",
    "            rank[p] = 0\n",
    "            front[0].append(p)\n",
    "            \n",
    "    i = 0\n",
    "    while front[i]:\n",
    "        nextFront = []\n",
    "        for p in front[i]:\n",
    "            for q in S[p]:\n",
    "                n[q] = n[q] - 1\n",
    "                if n[q] == 0:\n",
    "                    rank[q] = i + 1\n",
    "                    nextFront.append(q)\n",
    "        i = i + 1\n",
    "        front.append(nextFront)\n",
    "\n",
    "    del front[len(front) - 1]\n",
    "    \n",
    "    # Initialize crowding distances as zeros\n",
    "    crowding_distances = [0] * len(values)\n",
    "    \n",
    "    for front_solutions in front:\n",
    "        current_front_distances = crowding_distance_assignment(front_solutions, values)\n",
    "        for j, solution in enumerate(front_solutions):\n",
    "            crowding_distances[solution] = current_front_distances[solution]\n",
    "    \n",
    "    return rank, crowding_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of arrays, this funciton randomly paris them in groups of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_pairs(list_of_arrays, n):\n",
    "    # Generate all possible pairs\n",
    "    random.seed(2020)\n",
    "    all_pairs = list(itertools.combinations(list_of_arrays, 2))\n",
    "\n",
    "    # Randomly select n pairs\n",
    "    random_pairs = random.sample(all_pairs, n)\n",
    "\n",
    "    return random_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function execute one generation of genetic algorithm. It takes parameters:\n",
    "- interaction: the original interaction list\n",
    "- candidates: the candidate CFs from the last generation\n",
    "- model: the model to be explained\n",
    "- target: the target item\n",
    "- k: the top k items to consider. When target item drops out of top k recommendation list, the algorithm gives a valid explaination\n",
    "- yloss_cache: a dictionary to store the y-loss values of candidates already explored. This is a dynamic programming strategy to speed up the execution process\n",
    "- crossover_p: the probability of crossover process\n",
    "- mutation_p: the probability of mutation process\n",
    "- budget: the budget set for each explaination\n",
    "\n",
    "The algorithm runs in the following stages:\n",
    "- Pair all candidate CFs from the last generation\n",
    "- Do crossover and mutation for each pair\n",
    "- Compute losses of both old and new generation candidate CFs\n",
    "- Check if there is an explaination found\n",
    "    - If found, return the valid explainations\n",
    "    - If not found:\n",
    "        - Rank all the candidate by NSGA-II multi-objective losses\n",
    "        - Return the top half of candidate CFs and new candidate CFs for this new generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(interaction, candidates, model, target, k, yloss_cache, crossover_p, mutation_p, budget):\n",
    "    # print(len(candidates))\n",
    "    pairs = generate_random_pairs(candidates, len(candidates)//2)\n",
    "    t1 = time.time()\n",
    "    for first, second in pairs:\n",
    "        first, second = crossover(first, second, crossover_p)\n",
    "        first = mutate_array(interaction, first, mutation_p)\n",
    "        second = mutate_array(interaction, second, mutation_p)\n",
    "        first = remove_duplicates(first)\n",
    "        second = remove_duplicates(second)\n",
    "        candidates.append(first)\n",
    "        candidates.append(second)\n",
    "    t2 = time.time()\n",
    "    # print(\"Time taken for crossover and mutation: \", t2-t1)\n",
    "    # print(len(candidates))\n",
    "    t3 = time.time()\n",
    "    losses = [compute_loss(interaction, arr, model, target, k, yloss_cache) for arr in candidates]\n",
    "    # print(candidates)\n",
    "    t4 = time.time()\n",
    "    # print(\"Time taken for computing losses: \", t4-t3)\n",
    "    budget -= len(candidates)\n",
    "    # print(losses)\n",
    "    solved = False\n",
    "    solved_list = []\n",
    "    for i in range(len(losses)):\n",
    "        if losses[i][0] == 0:\n",
    "            solved = True\n",
    "            solved_list.append(candidates[i])\n",
    "    if solved:\n",
    "        return solved_list, solved, budget\n",
    "    ranks, crowding_distances = fast_nondominated_sort(losses)\n",
    "    # print(ranks)\n",
    "    candidates_with_metrics = list(zip(candidates, ranks, crowding_distances))\n",
    "\n",
    "    # Sort based on ranks (ascending) and then crowding distances (descending)\n",
    "    candidates_with_metrics.sort(key=lambda x: (x[1], -x[2]))\n",
    "\n",
    "    # Extract candidates after sorting\n",
    "    sorted_candidates = [pair[0] for pair in candidates_with_metrics]\n",
    "\n",
    "    # Extract the top third of candidates\n",
    "    least_loss_arrays = sorted_candidates[:len(sorted_candidates)//2]\n",
    "\n",
    "    return least_loss_arrays, solved, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, test_interaction, rank, sublists_info, top_k, crossover_p, mutation_p, budget):\n",
    "    target = get_position_item(model, test_interaction, rank)\n",
    "    new_gen = generate_random_sublists(test_interaction, sublists_info)\n",
    "    solved = False\n",
    "    yloss_cache = {}\n",
    "    while solved is not True:\n",
    "        new_gen, solved, budget = generation(test_interaction, new_gen, model, target, top_k, yloss_cache, crossover_p, mutation_p, budget)\n",
    "        if budget <= 0:\n",
    "            break\n",
    "    return new_gen, budget, solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Search\n",
    "This section implements brute force search to find hard cases for testing and demostration. Here, hard cases means that it has to remove more than 3 items from the original interaction list to find at least one CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def subsets_of_array(arr, k):\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        raise ValueError(\"Input must be a numpy array\")\n",
    "\n",
    "    n = len(arr)\n",
    "    subsets = []\n",
    "\n",
    "    for size in range(n-1, n-k-1, -1):\n",
    "        combinations = itertools.combinations(arr, size)\n",
    "        for combo in combinations:\n",
    "            subsets.append(np.array(combo))\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_brute(new_cf, model, target_item, top_k, yloss_cache):\n",
    "    cache_key = frozenset(new_cf)\n",
    "    if cache_key in yloss_cache:\n",
    "        yloss = yloss_cache[cache_key]\n",
    "    else:\n",
    "        new_prediction = model.predict(new_cf)\n",
    "        new_prediction[new_cf] = -StaticVars.FLOAT_MAX\n",
    "        new_rk_data = st.rankdata(-new_prediction, method='ordinal')\n",
    "\n",
    "        top_k_index = np.where(new_rk_data == top_k)[0][0]\n",
    "        yloss = compute_yloss(new_prediction[target_item], new_prediction[top_k_index])\n",
    "        yloss_cache[cache_key] = yloss\n",
    "    return yloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute(candidates, model, target, top_k, yloss_cache):\n",
    "    losses = [compute_loss_brute(arr, model, target, top_k, yloss_cache) for arr in candidates]\n",
    "    solved = False\n",
    "    for i in range(len(losses)):\n",
    "        if losses[i] == 0:\n",
    "            solved = True\n",
    "    return solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_main(model, test_interaction, rank, top_k):\n",
    "    target = get_position_item(model, test_interaction, rank)\n",
    "    new_gen = subsets_of_array(test_interaction, 3)\n",
    "    solved = False\n",
    "    yloss_cache = {}\n",
    "    solved = brute(new_gen, model, target, top_k, yloss_cache)\n",
    "    return solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard cases are stored in the file `final_list.txt` with first column the instance index and the second column the position of the target item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_list = []\n",
    "# for i in range(1000):\n",
    "    # test_interaction = test.sequences[i].copy()\n",
    "    # est_interaction = test_interaction[test_interaction != 0]\n",
    "    # test_interaction.sort()\n",
    "#     for j in range(1, 11):\n",
    "#         solved = brute_main(pooling_model, test_interaction, j, 10)\n",
    "#         if not solved:\n",
    "#             print(i, j)\n",
    "#             final_list.append((i, j))\n",
    "\n",
    "# with open('final_list.txt', 'w') as file:\n",
    "#     for item in final_list:\n",
    "#         file.write(f\"{item[0]}, {item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing hard case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code loops through cossover_p and mutation_p from 0.1 to 0.5 with 0.1 incremental to tune the two parameters using the first 50 hard cases found in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_crossover_p = [0.3, 0.4, 0.5]\n",
    "# test_mutation_p = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# result = []\n",
    "\n",
    "# for crossover_p in test_crossover_p:\n",
    "#     for mutation_p in test_mutation_p:\n",
    "#         t0 = time.time()\n",
    "#         solved_count = 0\n",
    "#         with open(\"final_list.txt\", 'r') as file:\n",
    "#             counter = 0\n",
    "#             for line in file:\n",
    "#                 if counter == 50:\n",
    "#                     break\n",
    "#                 i, j = map(int, line.split(','))\n",
    "#                 test_interaction = test.sequences[i].copy()\n",
    "#                 test_interaction = test_interaction[test_interaction != 0]\n",
    "#                 test_interaction.sort()\n",
    "#                 length_interaction = len(test_interaction)\n",
    "#                 if length_interaction <= 1:\n",
    "#                     continue\n",
    "#                 elif length_interaction < 5:\n",
    "#                     sublists_info = {\n",
    "#                         length_interaction - 1: length_interaction\n",
    "#                     }\n",
    "#                 else:\n",
    "#                     sublists_info ={\n",
    "#                         length_interaction - 1: length_interaction//2,\n",
    "#                         length_interaction - 2: length_interaction * (length_interaction - 1) // 8,\n",
    "#                         length_interaction - 3: length_interaction//3,\n",
    "#                         length_interaction - 4: length_interaction//4\n",
    "#                     }\n",
    "#                 # print(sublists_info)\n",
    "#                 output, budget, solved = main(pooling_model, test_interaction, j, sublists_info, 10, crossover_p, mutation_p, 1000)\n",
    "#                 if solved:\n",
    "#                     solved_count += 1\n",
    "#                 print(budget)\n",
    "#                 counter+=1\n",
    "#         t1 = time.time()\n",
    "#         result.append((crossover_p, mutation_p, t1-t0, solved_count))\n",
    "#         print(result)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the test above, a acomparative optimal result comes from parameter with crossover_p = 0.3 and mutation_p = 0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find CFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function returns the longest array among a list of arrays. Here, it is used to find the most wanted CFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def longest_array(arrays):\n",
    "    if not arrays:\n",
    "        return None  # Return None if the input list is empty\n",
    "    \n",
    "    max_length = 0\n",
    "    longest_arr = None\n",
    "    \n",
    "    for arr in arrays:\n",
    "        if arr.size > max_length:\n",
    "            max_length = arr.size\n",
    "            longest_arr = arr\n",
    "            \n",
    "    return longest_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying the most optimal crossover_p and mutation_p, the following function provides the same functionality as other strategies already implemented in `budget_strategies.ipynb` so it is comparable to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def find_cf(test, model, targets, no_users, budget):\n",
    "    cfs = dict.fromkeys(targets)\n",
    "    \n",
    "    # Progress bar for target loop\n",
    "    for target in tqdm(targets, desc=\"target position loop\", ncols=100):\n",
    "        cfs[target] = []\n",
    "        \n",
    "        # Progress bar for users loop\n",
    "        for i in trange(1, no_users + 1, desc='users loop', leave=False):\n",
    "        # for i in tqdm(range(no_users), desc=\"users loop\", ncols=100):\n",
    "            # print(i)\n",
    "            test_interaction = test.sequences[i].copy()\n",
    "            test_interaction = test_interaction[test_interaction != 0]\n",
    "            test_interaction.sort()\n",
    "            length_interaction = len(test_interaction)\n",
    "            if length_interaction <= 1:\n",
    "                continue\n",
    "            elif length_interaction < 5:\n",
    "                sublists_info = {\n",
    "                    length_interaction - 1: length_interaction\n",
    "                }\n",
    "            else:\n",
    "                sublists_info ={\n",
    "                    length_interaction - 1: length_interaction//2,\n",
    "                    length_interaction - 2: length_interaction * (length_interaction - 1) // 10,\n",
    "                    length_interaction - 3: length_interaction//3,\n",
    "                    length_interaction - 4: length_interaction//4\n",
    "                }\n",
    "            # print(sublists_info)\n",
    "            output, remain_budget, solved = main(model, test_interaction, target, sublists_info, 10, 0.3, 0.2, budget)\n",
    "            # print(remain_budget)\n",
    "            if solved:\n",
    "                # print(output)\n",
    "                cf = longest_array(output)\n",
    "                cfs[target].extend(cf)\n",
    "            else:\n",
    "                cfs[target].append(None)\n",
    "                \n",
    "    return cfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test case of looping target positions of 1, 3, 5, 7 with 1000 user instances. Each case has a budget of 1000.\n",
    "\n",
    "However, note that genetic algorithm might not have advantages on simple cases since it has heavy work loads in each generation. It works better for hard cases. Therefore, its performance is not as good as others.\n",
    "\n",
    "Also, by turning how many initial candidate to generate in the begining might help in improving the performance as well. This is not yet implemented in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "target position loop:   0%|                                                   | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d5b221e6da4fb1b625b167e49a480d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "users loop:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "target position loop:   0%|                                                   | 0/4 [01:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m find_cf(test, pooling_model, [\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m7\u001b[39;49m], no_users\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, budget\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[83], line 32\u001b[0m, in \u001b[0;36mfind_cf\u001b[0;34m(test, model, targets, no_users, budget)\u001b[0m\n\u001b[1;32m     25\u001b[0m     sublists_info \u001b[39m=\u001b[39m{\n\u001b[1;32m     26\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m: length_interaction\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     27\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m: length_interaction \u001b[39m*\u001b[39m (length_interaction \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[1;32m     28\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m3\u001b[39m: length_interaction\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     29\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m: length_interaction\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m\n\u001b[1;32m     30\u001b[0m     }\n\u001b[1;32m     31\u001b[0m \u001b[39m# print(sublists_info)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m output, remain_budget, solved \u001b[39m=\u001b[39m main(model, test_interaction, target, sublists_info, \u001b[39m10\u001b[39;49m, \u001b[39m0.3\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, budget)\n\u001b[1;32m     33\u001b[0m \u001b[39m# print(remain_budget)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m solved:\n\u001b[1;32m     35\u001b[0m     \u001b[39m# print(output)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model, test_interaction, rank, sublists_info, top_k, crossover_p, mutation_p, budget)\u001b[0m\n\u001b[1;32m      5\u001b[0m yloss_cache \u001b[39m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m \u001b[39mwhile\u001b[39;00m solved \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     new_gen, solved, budget \u001b[39m=\u001b[39m generation(test_interaction, new_gen, model, target, top_k, yloss_cache, crossover_p, mutation_p, budget)\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m budget \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      9\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 17\u001b[0m, in \u001b[0;36mgeneration\u001b[0;34m(interaction, candidates, model, target, k, yloss_cache, crossover_p, mutation_p, budget)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(\"Time taken for crossover and mutation: \", t2-t1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(len(candidates))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m t3 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 17\u001b[0m losses \u001b[39m=\u001b[39m [compute_loss(interaction, arr, model, target, k, yloss_cache) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m candidates]\n\u001b[1;32m     18\u001b[0m \u001b[39m# print(candidates)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m t4 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[74], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(\"Time taken for crossover and mutation: \", t2-t1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(len(candidates))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m t3 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 17\u001b[0m losses \u001b[39m=\u001b[39m [compute_loss(interaction, arr, model, target, k, yloss_cache) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m candidates]\n\u001b[1;32m     18\u001b[0m \u001b[39m# print(candidates)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m t4 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[71], line 45\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(old_cf, new_cf, model, target_item, top_k, yloss_cache)\u001b[0m\n\u001b[1;32m     43\u001b[0m subset_prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(superset)\n\u001b[1;32m     44\u001b[0m subset_prediction[superset] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mStaticVars\u001b[39m.\u001b[39mFLOAT_MAX\n\u001b[0;32m---> 45\u001b[0m sub_rk_data \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39;49mrankdata(\u001b[39m-\u001b[39;49msubset_prediction, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mordinal\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m sub_top_k_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(sub_rk_data \u001b[39m==\u001b[39m top_k)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m subset_yloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m compute_yloss(subset_prediction[target_item], subset_prediction[sub_top_k_index])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:9505\u001b[0m, in \u001b[0;36mrankdata\u001b[0;34m(a, method, axis, nan_policy)\u001b[0m\n\u001b[1;32m   9502\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mfull_like(arr, np\u001b[39m.\u001b[39mnan)\n\u001b[1;32m   9504\u001b[0m algo \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmergesort\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mordinal\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mquicksort\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 9505\u001b[0m sorter \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margsort(arr, kind\u001b[39m=\u001b[39;49malgo)\n\u001b[1;32m   9507\u001b[0m inv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(sorter\u001b[39m.\u001b[39msize, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)\n\u001b[1;32m   9508\u001b[0m inv[sorter] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(sorter\u001b[39m.\u001b[39msize, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1146\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1039\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margsort\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1040\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \n\u001b[1;32m   1145\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margsort\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "find_cf(test, pooling_model, [1, 3, 5, 7], no_users=1000, budget=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
