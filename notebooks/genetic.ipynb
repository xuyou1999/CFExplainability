{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats as st\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_state = np.random.RandomState(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently working directory\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# load functions from other notebooks\n",
    "helpers_file = os.path.join(base_dir, 'helpers.ipynb')\n",
    "%run $helpers_file\n",
    "\n",
    "# Load spotlight module\n",
    "for p in ['../spotlight_ext']:\n",
    "    module_path = os.path.abspath(os.path.join(base_dir, p))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model(model_type='entire')\n",
    "pooling_model = load_model('pooling')\n",
    "\n",
    "pretrained_models = {\n",
    "    'lstm': lstm_model,\n",
    "    'pooling': pooling_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "\n",
    "# get dataset\n",
    "dataset = get_movielens_dataset(variant='1M')\n",
    "train, test = random_train_test_split(dataset, random_state=random_state)\n",
    "\n",
    "max_sequence_length = 20\n",
    "train = train.to_sequence(max_sequence_length=max_sequence_length)\n",
    "test = test.to_sequence(max_sequence_length=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target item is 14 in this test case, top k is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([197, 124, 125, 190, 114, 186, 196,  59, 200, 191, 177], dtype=int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interaction = test.sequences[test.user_ids == 3][0].copy()\n",
    "test_interaction = test_interaction[test_interaction != 0]\n",
    "test_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_test_interaction = len(test_interaction)\n",
    "len_test_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = lstm_model.predict(test_interaction)\n",
    "prediction[test_interaction] = -StaticVars.FLOAT_MAX\n",
    "rk_data = st.rankdata(-prediction, method='ordinal')\n",
    "index = np.where(rk_data == 2)\n",
    "index[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random CF candidate selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_sublists(original_list, sublists_info):\n",
    "    result_sublists = []\n",
    "    rng = np.random.default_rng(seed=2020)  # Seed for reproducibility\n",
    "\n",
    "    for length, count in sublists_info.items():\n",
    "        generated_sublists_for_length = set()\n",
    "\n",
    "        while len(generated_sublists_for_length) < count:\n",
    "            sublist = tuple(rng.choice(original_list, length, replace=False))\n",
    "            generated_sublists_for_length.add(sublist)\n",
    "\n",
    "        result_sublists.extend(np.array(list(sublist)) for sublist in generated_sublists_for_length)\n",
    "\n",
    "    return result_sublists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublists_info ={\n",
    "    5:5,\n",
    "    6:5,\n",
    "    7:5,\n",
    "    8:5,\n",
    "    9:5,\n",
    "    10:5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_gen = generate_random_sublists(test_interaction, sublists_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 59, 197, 190, 196, 200], dtype=int32),\n",
       " array([190, 186, 196, 197,  59], dtype=int32),\n",
       " array([125, 190, 177,  59, 186], dtype=int32),\n",
       " array([190, 197, 124, 191, 196], dtype=int32),\n",
       " array([200, 125, 196, 124, 191], dtype=int32),\n",
       " array([197, 124, 196, 177, 191, 186], dtype=int32),\n",
       " array([197, 190,  59, 124, 177, 191], dtype=int32),\n",
       " array([196, 177, 197, 125, 114,  59], dtype=int32),\n",
       " array([191, 190, 186, 114, 200, 197], dtype=int32),\n",
       " array([ 59, 200, 177, 191, 114, 186], dtype=int32),\n",
       " array([191, 177,  59, 124, 196, 186, 125], dtype=int32),\n",
       " array([125, 196, 124, 114, 197, 191, 177], dtype=int32),\n",
       " array([114, 177, 125, 196, 124, 186,  59], dtype=int32),\n",
       " array([124, 186, 177, 196, 200,  59, 190], dtype=int32),\n",
       " array([186, 191, 125, 190, 114, 200, 197], dtype=int32),\n",
       " array([124, 125, 177,  59, 190, 186, 114, 197], dtype=int32),\n",
       " array([197, 196, 125, 191, 124, 177, 114, 186], dtype=int32),\n",
       " array([125, 124, 197, 196, 177, 190, 200,  59], dtype=int32),\n",
       " array([196, 124, 197, 190, 200, 191, 125, 177], dtype=int32),\n",
       " array([114, 191, 124, 125, 196, 197,  59, 177], dtype=int32),\n",
       " array([200, 196, 190, 197, 114, 177, 186,  59, 191], dtype=int32),\n",
       " array([177, 191, 190, 125, 197, 186, 114, 124, 196], dtype=int32),\n",
       " array([190, 124, 191, 200, 196, 125, 114, 186, 197], dtype=int32),\n",
       " array([125,  59, 114, 190, 124, 191, 186, 197, 177], dtype=int32),\n",
       " array([196, 125, 177, 124, 191, 114, 186,  59, 200], dtype=int32),\n",
       " array([191, 200, 197, 196, 186,  59, 124, 177, 190, 114], dtype=int32),\n",
       " array([200, 177, 186, 190, 125, 191,  59, 196, 124, 197], dtype=int32),\n",
       " array([186, 190,  59, 177, 196, 191, 114, 125, 124, 200], dtype=int32),\n",
       " array([200, 190, 191, 197, 186, 177, 125,  59, 196, 114], dtype=int32),\n",
       " array([ 59, 190, 114, 177, 200, 191, 197, 124, 186, 196], dtype=int32)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossover and Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def crossover(first_list, second_list, p_cross):\n",
    "    # Find the shorter length among the two lists\n",
    "    length_first = len(first_list)\n",
    "    length_second = len(second_list)\n",
    "    shorter_length = min(length_first, length_second)\n",
    "    \n",
    "    # Compute the number of crossover points\n",
    "    num_crossovers = int(shorter_length * p_cross)\n",
    "    \n",
    "    # Choose random indices for crossover within the range of shorter length\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    crossover_indices_first = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    crossover_indices_second = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    \n",
    "    # Sort the crossover indices\n",
    "    crossover_indices_first.sort()\n",
    "    crossover_indices_second.sort()\n",
    "    \n",
    "    # Swap the elements at the crossover indices\n",
    "    for i in range(num_crossovers):\n",
    "        index_first = crossover_indices_first[i]\n",
    "        index_second = crossover_indices_second[i]\n",
    "        first_list[index_first], second_list[index_second] = second_list[index_second], first_list[index_first]\n",
    "    \n",
    "    return first_list, second_list\n",
    "\n",
    "def mutate_array(org_arr, arr_to_mutate, mutation_probability):\n",
    "    # Calculate the number of elements to mutate\n",
    "    num_mutations = int(mutation_probability * len(arr_to_mutate))\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    # Select the indices to mutate\n",
    "    indices_to_mutate = rng.choice(range(len(arr_to_mutate)), size=num_mutations, replace=False)\n",
    "    \n",
    "    # Mutate the selected elements\n",
    "    for idx in indices_to_mutate:\n",
    "        arr_to_mutate[idx] = rng.choice(org_arr)\n",
    "\n",
    "    return arr_to_mutate\n",
    "\n",
    "\n",
    "def remove_duplicates(arr):\n",
    "    _, idx = np.unique(arr, return_index=True)\n",
    "    return arr[np.sort(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([186, 190, 114, 177,  59, 191, 197, 196], dtype=int32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first, second = crossover(first_gen[-1], first_gen[-2], 0.3)\n",
    "first = mutate_array(test_interaction, first, 0.2)\n",
    "first = remove_duplicates(first)\n",
    "second = remove_duplicates(second)\n",
    "first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "import numpy as np\n",
    "\n",
    "def all_subsets(arr):\n",
    "    return chain.from_iterable(combinations(arr, r) for r in range(1, len(arr)))\n",
    "\n",
    "def compute_yloss(target_score, kth_score):\n",
    "    # print(target_score, kth_score)\n",
    "    yloss = max(0, target_score / kth_score - 1.0)\n",
    "    return yloss\n",
    "\n",
    "def compute_distance(x, y):\n",
    "    diff = np.setdiff1d(x, y)\n",
    "    return len(diff)\n",
    "\n",
    "def compute_loss(old_cf, new_cf, model, target_item, top_k):\n",
    "    new_prediction = model.predict(new_cf)\n",
    "    new_prediction[new_cf] = -StaticVars.FLOAT_MAX\n",
    "    new_rk_data = st.rankdata(-new_prediction, method='ordinal')\n",
    "\n",
    "    top_k_index = np.where(new_rk_data == top_k)[0][0]\n",
    "    yloss = compute_yloss(new_prediction[target_item], new_prediction[top_k_index])\n",
    "    dis = compute_distance(old_cf, new_cf)\n",
    "\n",
    "    subset_yloss = 0\n",
    "    for subset in all_subsets(new_cf):\n",
    "        subset = np.array(subset)\n",
    "        sub_prediction = model.predict(subset)\n",
    "        sub_prediction[subset] = -StaticVars.FLOAT_MAX\n",
    "        sub_rk_data = st.rankdata(-sub_prediction, method='ordinal')\n",
    "        sub_top_k_index = np.where(sub_rk_data == top_k)[0][0]\n",
    "        subset_yloss += compute_yloss(sub_prediction[target_item], sub_prediction[sub_top_k_index])\n",
    "\n",
    "    return (yloss, dis, subset_yloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3, 0.041111111640930176)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(test_interaction, first, lstm_model, 14, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
