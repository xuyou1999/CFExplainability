{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats as st\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_state = np.random.RandomState(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently working directory\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# load functions from other notebooks\n",
    "helpers_file = os.path.join(base_dir, 'helpers.ipynb')\n",
    "%run $helpers_file\n",
    "\n",
    "# Load spotlight module\n",
    "for p in ['../spotlight_ext']:\n",
    "    module_path = os.path.abspath(os.path.join(base_dir, p))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model(model_type='entire')\n",
    "pooling_model = load_model('pooling')\n",
    "\n",
    "pretrained_models = {\n",
    "    'lstm': lstm_model,\n",
    "    'pooling': pooling_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "\n",
    "# get dataset\n",
    "dataset = get_movielens_dataset(variant='1M')\n",
    "train, test = random_train_test_split(dataset, random_state=random_state)\n",
    "\n",
    "max_sequence_length = 20\n",
    "train = train.to_sequence(max_sequence_length=max_sequence_length)\n",
    "test = test.to_sequence(max_sequence_length=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59, 114, 124, 125, 177, 186, 190, 191, 196, 197, 200], dtype=int32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interaction = test.sequences[test.user_ids == 3][0].copy()\n",
    "test_interaction = test_interaction[test_interaction != 0]\n",
    "test_interaction.sort()\n",
    "test_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_test_interaction = len(test_interaction)\n",
    "len_test_interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a specific instance, this function takes a position and gives the item id in that position of the predicted item list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_item(model, test_interaction, position=1):\n",
    "    prediction = model.predict(test_interaction)\n",
    "    prediction[test_interaction] = -StaticVars.FLOAT_MAX\n",
    "    rk_data = st.rankdata(-prediction, method='ordinal')\n",
    "    index = np.where(rk_data == position)\n",
    "    return index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_position_item(pooling_model, test_interaction, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random CF candidate selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function randomly generates a set of subsets of a specific array. The amount of array to be generated is defined by \"sublists_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_sublists(original_list, sublists_info):\n",
    "    result_sublists = []\n",
    "    rng = np.random.default_rng(seed=2020)  # Seed for reproducibility\n",
    "\n",
    "    for length, count in sublists_info.items():\n",
    "        generated_sublists_for_length = set()\n",
    "\n",
    "        while len(generated_sublists_for_length) < count:\n",
    "            sublist = tuple(rng.choice(original_list, length, replace=False))\n",
    "            generated_sublists_for_length.add(sublist)\n",
    "\n",
    "        result_sublists.extend(np.array(list(sublist)) for sublist in generated_sublists_for_length)\n",
    "\n",
    "    return result_sublists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossover and Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions perform crossover and mutation job. The crossover function takes two parents and returns two children. The mutation function takes a parent and returns a child. The number of elements to crossover and mutate is defiend by the probablity parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def crossover(first_list, second_list, p_cross):\n",
    "    # Find the shorter length among the two lists\n",
    "    length_first = len(first_list)\n",
    "    length_second = len(second_list)\n",
    "    shorter_length = min(length_first, length_second)\n",
    "    \n",
    "    # Compute the number of crossover points\n",
    "    num_crossovers = int(shorter_length * p_cross)\n",
    "    \n",
    "    # Choose random indices for crossover\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    crossover_indices_first = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    crossover_indices_second = rng.choice(shorter_length, num_crossovers, replace=False)\n",
    "    \n",
    "    # Sort the crossover indices\n",
    "    crossover_indices_first.sort()\n",
    "    crossover_indices_second.sort()\n",
    "    \n",
    "    # Swap the elements at the crossover indices\n",
    "    for i in range(num_crossovers):\n",
    "        index_first = crossover_indices_first[i]\n",
    "        index_second = crossover_indices_second[i]\n",
    "        first_list[index_first], second_list[index_second] = second_list[index_second], first_list[index_first]\n",
    "    \n",
    "    return first_list, second_list\n",
    "\n",
    "def mutate_array(org_arr, arr_to_mutate, mutation_probability):\n",
    "    # Calculate the number of elements to mutate\n",
    "    num_mutations = int(mutation_probability * len(arr_to_mutate))\n",
    "    rng = np.random.default_rng(seed=2020)\n",
    "    # Select the indices to mutate\n",
    "    indices_to_mutate = rng.choice(range(len(arr_to_mutate)), size=num_mutations, replace=False)\n",
    "    \n",
    "    # Mutate the selected elements\n",
    "    for idx in indices_to_mutate:\n",
    "        arr_to_mutate[idx] = rng.choice(org_arr)\n",
    "\n",
    "    return arr_to_mutate\n",
    "\n",
    "\n",
    "def remove_duplicates(arr):\n",
    "    _, idx = np.unique(arr, return_index=True)\n",
    "    return arr[np.sort(idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three loss functions defined: y-loss, distance, and y-loss for subsets of removing items (supersets of candidate CFs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "class StaticVars:\n",
    "    FLOAT_MAX = float('inf')\n",
    "\n",
    "def supersets_of_new_subsets_of_old(new_cf, old_cf):\n",
    "    diff = np.setdiff1d(old_cf, new_cf)  # Elements that are in old_cf but not in new_cf\n",
    "    for r in range(1, len(diff) + 1):\n",
    "        for subset in combinations(diff, r):\n",
    "            yield np.union1d(new_cf, subset)\n",
    "\n",
    "def compute_yloss(target_score, kth_score):\n",
    "    yloss = max(0, target_score / kth_score - 1.0)\n",
    "    return yloss\n",
    "\n",
    "def compute_distance(x, y):\n",
    "    diff = np.setdiff1d(x, y)\n",
    "    return len(diff)\n",
    "\n",
    "def compute_loss(old_cf, new_cf, model, target_item, top_k, yloss_cache):\n",
    "    cache_key = frozenset(new_cf)\n",
    "    if cache_key in yloss_cache:\n",
    "        yloss = yloss_cache[cache_key]\n",
    "    else:\n",
    "        new_prediction = model.predict(new_cf)\n",
    "        new_prediction[new_cf] = -StaticVars.FLOAT_MAX\n",
    "        new_rk_data = st.rankdata(-new_prediction, method='ordinal')\n",
    "\n",
    "        top_k_index = np.where(new_rk_data == top_k)[0][0]\n",
    "        yloss = compute_yloss(new_prediction[target_item], new_prediction[top_k_index])\n",
    "        yloss_cache[cache_key] = yloss\n",
    "    dis = compute_distance(old_cf, new_cf)\n",
    "\n",
    "    subset_yloss = 0\n",
    "    for superset in supersets_of_new_subsets_of_old(new_cf, old_cf):\n",
    "        cache_key = frozenset(superset)\n",
    "        if cache_key in yloss_cache:\n",
    "            # print(\"Cache hit\")\n",
    "            subset_yloss += yloss_cache[cache_key]\n",
    "        else:\n",
    "            subset_prediction = model.predict(superset)\n",
    "            subset_prediction[superset] = -StaticVars.FLOAT_MAX\n",
    "            sub_rk_data = st.rankdata(-subset_prediction, method='ordinal')\n",
    "            sub_top_k_index = np.where(sub_rk_data == top_k)[0][0]\n",
    "            subset_yloss += compute_yloss(subset_prediction[target_item], subset_prediction[sub_top_k_index])\n",
    "            yloss_cache[cache_key] = subset_yloss\n",
    "\n",
    "    return list([yloss, dis, subset_yloss])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSGA-II\n",
    "Apply NSGA-II to the problem of finding the optimal candicates in multi-objective optimization problem.\n",
    "Based on:\n",
    "- Non-domination Rank\n",
    "- Crowding Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominates(row, candidateRow):\n",
    "    \"\"\"Determine if one solution dominates another\"\"\"\n",
    "    return all(r <= cr for r, cr in zip(row, candidateRow)) and any(r < cr for r, cr in zip(row, candidateRow))\n",
    "\n",
    "def crowding_distance_assignment(front, values):\n",
    "    distances = [0] * len(values)  # Initialize the distance for every solution as 0\n",
    "    num_objs = len(values[0])\n",
    "    \n",
    "    for m in range(num_objs):\n",
    "        sorted_front = sorted(front, key=lambda x: values[x][m])\n",
    "\n",
    "        # Assign infinite distance at boundaries.\n",
    "        distances[sorted_front[0]] = distances[sorted_front[-1]] = float('inf')\n",
    "\n",
    "        # Normalize the objective values for distance computation.\n",
    "        obj_min = values[sorted_front[0]][m]\n",
    "        obj_max = values[sorted_front[-1]][m]\n",
    "        denom = obj_max - obj_min if obj_max != obj_min else 1\n",
    "\n",
    "        for i in range(1, len(sorted_front) - 1):\n",
    "            distances[sorted_front[i]] += (values[sorted_front[i + 1]][m] - values[sorted_front[i - 1]][m]) / denom\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "\n",
    "def fast_nondominated_sort(values):\n",
    "    \"\"\"NSGA-II's fast non-dominated sort\"\"\"\n",
    "    S = [[] for _ in range(len(values))]\n",
    "    front = [[]]\n",
    "    n = [0 for _ in range(len(values))]\n",
    "    rank = [-1 for _ in range(len(values))]\n",
    "    \n",
    "    for p in range(len(values)):\n",
    "        S[p] = []\n",
    "        n[p] = 0\n",
    "        for q in range(len(values)):\n",
    "            if dominates(values[p], values[q]):\n",
    "                S[p].append(q)\n",
    "            elif dominates(values[q], values[p]):\n",
    "                n[p] += 1\n",
    "        if n[p] == 0:\n",
    "            rank[p] = 0\n",
    "            front[0].append(p)\n",
    "            \n",
    "    i = 0\n",
    "    while front[i]:\n",
    "        nextFront = []\n",
    "        for p in front[i]:\n",
    "            for q in S[p]:\n",
    "                n[q] = n[q] - 1\n",
    "                if n[q] == 0:\n",
    "                    rank[q] = i + 1\n",
    "                    nextFront.append(q)\n",
    "        i = i + 1\n",
    "        front.append(nextFront)\n",
    "\n",
    "    del front[len(front) - 1]\n",
    "    \n",
    "    # Initialize crowding distances as zeros\n",
    "    crowding_distances = [0] * len(values)\n",
    "    \n",
    "    for front_solutions in front:\n",
    "        current_front_distances = crowding_distance_assignment(front_solutions, values)\n",
    "        for j, solution in enumerate(front_solutions):\n",
    "            crowding_distances[solution] = current_front_distances[solution]\n",
    "    \n",
    "    return rank, crowding_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_pairs(list_of_arrays, n):\n",
    "    # Generate all possible pairs\n",
    "    random.seed(2020)\n",
    "    all_pairs = list(itertools.combinations(list_of_arrays, 2))\n",
    "\n",
    "    # Randomly select n pairs\n",
    "    random_pairs = random.sample(all_pairs, n)\n",
    "\n",
    "    return random_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(interaction, candidates, model, target, k, yloss_cache, crossover_p, mutation_p, budget):\n",
    "    # print(len(candidates))\n",
    "    pairs = generate_random_pairs(candidates, len(candidates)//2)\n",
    "    t1 = time.time()\n",
    "    for first, second in pairs:\n",
    "        first, second = crossover(first, second, crossover_p)\n",
    "        first = mutate_array(interaction, first, mutation_p)\n",
    "        second = mutate_array(interaction, second, mutation_p)\n",
    "        first = remove_duplicates(first)\n",
    "        second = remove_duplicates(second)\n",
    "        candidates.append(first)\n",
    "        candidates.append(second)\n",
    "    t2 = time.time()\n",
    "    # print(\"Time taken for crossover and mutation: \", t2-t1)\n",
    "    # print(len(candidates))\n",
    "    t3 = time.time()\n",
    "    losses = [compute_loss(interaction, arr, model, target, k, yloss_cache) for arr in candidates]\n",
    "    # print(candidates)\n",
    "    t4 = time.time()\n",
    "    # print(\"Time taken for computing losses: \", t4-t3)\n",
    "    budget -= len(candidates)\n",
    "    # print(losses)\n",
    "    solved = False\n",
    "    solved_list = []\n",
    "    for i in range(len(losses)):\n",
    "        if losses[i][0] == 0:\n",
    "            solved = True\n",
    "            solved_list.append(candidates[i])\n",
    "    if solved:\n",
    "        return solved_list, solved, budget\n",
    "    ranks, crowding_distances = fast_nondominated_sort(losses)\n",
    "    # print(ranks)\n",
    "    candidates_with_metrics = list(zip(candidates, ranks, crowding_distances))\n",
    "\n",
    "    # Sort based on ranks (ascending) and then crowding distances (descending)\n",
    "    candidates_with_metrics.sort(key=lambda x: (x[1], -x[2]))\n",
    "\n",
    "    # Extract candidates after sorting\n",
    "    sorted_candidates = [pair[0] for pair in candidates_with_metrics]\n",
    "\n",
    "    # Extract the top third of candidates\n",
    "    least_loss_arrays = sorted_candidates[:len(sorted_candidates)//2]\n",
    "\n",
    "    return least_loss_arrays, solved, budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, test_interaction, rank, sublists_info, top_k, crossover_p, mutation_p, budget):\n",
    "    target = get_position_item(model, test_interaction, rank)\n",
    "    new_gen = generate_random_sublists(test_interaction, sublists_info)\n",
    "    solved = False\n",
    "    yloss_cache = {}\n",
    "    while solved is not True:\n",
    "        new_gen, solved, budget = generation(test_interaction, new_gen, model, target, top_k, yloss_cache, crossover_p, mutation_p, budget)\n",
    "        if budget <= 0:\n",
    "            break\n",
    "    return new_gen, budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Search\n",
    "Find hard cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def subsets_of_array(arr, k):\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        raise ValueError(\"Input must be a numpy array\")\n",
    "\n",
    "    n = len(arr)\n",
    "    subsets = []\n",
    "\n",
    "    for size in range(n-1, n-k-1, -1):\n",
    "        combinations = itertools.combinations(arr, size)\n",
    "        for combo in combinations:\n",
    "            subsets.append(np.array(combo))\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_brute(new_cf, model, target_item, top_k, yloss_cache):\n",
    "    cache_key = frozenset(new_cf)\n",
    "    if cache_key in yloss_cache:\n",
    "        yloss = yloss_cache[cache_key]\n",
    "    else:\n",
    "        new_prediction = model.predict(new_cf)\n",
    "        new_prediction[new_cf] = -StaticVars.FLOAT_MAX\n",
    "        new_rk_data = st.rankdata(-new_prediction, method='ordinal')\n",
    "\n",
    "        top_k_index = np.where(new_rk_data == top_k)[0][0]\n",
    "        yloss = compute_yloss(new_prediction[target_item], new_prediction[top_k_index])\n",
    "        yloss_cache[cache_key] = yloss\n",
    "    return yloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute(candidates, model, target, top_k, yloss_cache):\n",
    "    losses = [compute_loss_brute(arr, model, target, top_k, yloss_cache) for arr in candidates]\n",
    "    solved = False\n",
    "    for i in range(len(losses)):\n",
    "        if losses[i] == 0:\n",
    "            solved = True\n",
    "    return solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_main(model, test_interaction, rank, top_k):\n",
    "    target = get_position_item(model, test_interaction, rank)\n",
    "    new_gen = subsets_of_array(test_interaction, 3)\n",
    "    solved = False\n",
    "    yloss_cache = {}\n",
    "    solved = brute(new_gen, model, target, top_k, yloss_cache)\n",
    "    return solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_list = []\n",
    "# for i in range(1000):\n",
    "    # test_interaction = test.sequences[i].copy()\n",
    "    # est_interaction = test_interaction[test_interaction != 0]\n",
    "    # test_interaction.sort()\n",
    "#     for j in range(1, 11):\n",
    "#         solved = brute_main(pooling_model, test_interaction, j, 10)\n",
    "#         if not solved:\n",
    "#             print(i, j)\n",
    "#             final_list.append((i, j))\n",
    "\n",
    "# with open('final_list.txt', 'w') as file:\n",
    "#     for item in final_list:\n",
    "#         file.write(f\"{item[0]}, {item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing hard case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887\n",
      "887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m     sublists_info \u001b[39m=\u001b[39m{\n\u001b[1;32m     16\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m: length_interaction\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     17\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m: length_interaction \u001b[39m*\u001b[39m (length_interaction \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m8\u001b[39m,\n\u001b[1;32m     18\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m3\u001b[39m: length_interaction\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     19\u001b[0m         length_interaction \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m: length_interaction\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m\n\u001b[1;32m     20\u001b[0m     }\n\u001b[1;32m     21\u001b[0m \u001b[39m# print(sublists_info)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m output, budget \u001b[39m=\u001b[39m main(pooling_model, test_interaction, j, sublists_info, \u001b[39m10\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(budget)\n",
      "Cell \u001b[0;32mIn[185], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model, test_interaction, rank, sublists_info, top_k, crossover_p, mutation_p, budget)\u001b[0m\n\u001b[1;32m      5\u001b[0m yloss_cache \u001b[39m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m \u001b[39mwhile\u001b[39;00m solved \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     new_gen, solved, budget \u001b[39m=\u001b[39m generation(test_interaction, new_gen, model, target, top_k, yloss_cache, crossover_p, mutation_p, budget)\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m budget \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      9\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[184], line 17\u001b[0m, in \u001b[0;36mgeneration\u001b[0;34m(interaction, candidates, model, target, k, yloss_cache, crossover_p, mutation_p, budget)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(\"Time taken for crossover and mutation: \", t2-t1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(len(candidates))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m t3 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 17\u001b[0m losses \u001b[39m=\u001b[39m [compute_loss(interaction, arr, model, target, k, yloss_cache) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m candidates]\n\u001b[1;32m     18\u001b[0m \u001b[39m# print(candidates)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m t4 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[184], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(\"Time taken for crossover and mutation: \", t2-t1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(len(candidates))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m t3 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 17\u001b[0m losses \u001b[39m=\u001b[39m [compute_loss(interaction, arr, model, target, k, yloss_cache) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m candidates]\n\u001b[1;32m     18\u001b[0m \u001b[39m# print(candidates)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m t4 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[181], line 45\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(old_cf, new_cf, model, target_item, top_k, yloss_cache)\u001b[0m\n\u001b[1;32m     43\u001b[0m subset_prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(superset)\n\u001b[1;32m     44\u001b[0m subset_prediction[superset] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mStaticVars\u001b[39m.\u001b[39mFLOAT_MAX\n\u001b[0;32m---> 45\u001b[0m sub_rk_data \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39;49mrankdata(\u001b[39m-\u001b[39;49msubset_prediction, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mordinal\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m sub_top_k_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(sub_rk_data \u001b[39m==\u001b[39m top_k)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m subset_yloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m compute_yloss(subset_prediction[target_item], subset_prediction[sub_top_k_index])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:9505\u001b[0m, in \u001b[0;36mrankdata\u001b[0;34m(a, method, axis, nan_policy)\u001b[0m\n\u001b[1;32m   9502\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mfull_like(arr, np\u001b[39m.\u001b[39mnan)\n\u001b[1;32m   9504\u001b[0m algo \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmergesort\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mordinal\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mquicksort\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 9505\u001b[0m sorter \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margsort(arr, kind\u001b[39m=\u001b[39;49malgo)\n\u001b[1;32m   9507\u001b[0m inv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(sorter\u001b[39m.\u001b[39msize, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)\n\u001b[1;32m   9508\u001b[0m inv[sorter] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(sorter\u001b[39m.\u001b[39msize, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1146\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1039\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margsort\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1040\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \n\u001b[1;32m   1145\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margsort\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"final_list.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        i, j = map(int, line.split(','))\n",
    "        test_interaction = test.sequences[i].copy()\n",
    "        test_interaction = test_interaction[test_interaction != 0]\n",
    "        test_interaction.sort()\n",
    "        length_interaction = len(test_interaction)\n",
    "        if length_interaction <= 1:\n",
    "            continue\n",
    "        elif length_interaction < 5:\n",
    "            sublists_info = {\n",
    "                length_interaction - 1: length_interaction\n",
    "            }\n",
    "        else:\n",
    "            sublists_info ={\n",
    "                length_interaction - 1: length_interaction//2,\n",
    "                length_interaction - 2: length_interaction * (length_interaction - 1) // 8,\n",
    "                length_interaction - 3: length_interaction//3,\n",
    "                length_interaction - 4: length_interaction//4\n",
    "            }\n",
    "        # print(sublists_info)\n",
    "        output, budget = main(pooling_model, test_interaction, j, sublists_info, 10, 0.2, 0.2, 1000)\n",
    "        print(budget)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
