{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from itertools import combinations\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently working directory\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type='pooling', models_folder='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.load(os.path.join(base_dir, models_folder, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_type='pooling', models_folder='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.save(model, os.path.join(base_dir, models_folder, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticVars:\n",
    "    FLOAT_MAX = np.finfo(np.float32).max\n",
    "    INT_MAX = np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionsInfo:\n",
    "    score = 0\n",
    "#     interactions = []\n",
    "#     complete_interactions = []\n",
    "#     iter_found = -1\n",
    "    y_loss = 1.0\n",
    "    proximity_loss = StaticVars.FLOAT_MAX\n",
    "#     total_loss = StaticVars.FLOAT_MAX\n",
    "\n",
    "    def __init__(self, uid, iid, interactions, budget=1000, fobj=True, fconstraint=True):\n",
    "        self.user_id = uid\n",
    "        self.item_id = iid\n",
    "        self.available_budget = budget\n",
    "\n",
    "        self.satisfy_objective = fobj\n",
    "        self.satisfy_contraints = fconstraint\n",
    "\n",
    "        self.recommendation = None\n",
    "        self.interactions = dict(original=interactions, initial=[], best=[])\n",
    "        self.loss = dict(initial=StaticVars.FLOAT_MAX, best=StaticVars.FLOAT_MAX)\n",
    "        self.iter_no = dict(initial=budget, best=budget, total=budget)\n",
    "        self.budget_spent = dict(initial=budget, best=budget, total=budget)\n",
    "\n",
    "        self.solution_found = False\n",
    "        self.pos = StaticVars.INT_MAX\n",
    "        self.cfs_dist = len(interactions)\n",
    "        self.stats_per_cardinality = [0] * len(interactions)\n",
    "        self.max_updated_card = -1\n",
    "\n",
    "        self.len_interactions = len(self.interactions['original'])\n",
    "\n",
    "    def __str__(self):\n",
    "        sorted_recommended_items = [\n",
    "            (n[0], n[1].detach().numpy().flatten()[0]) if isinstance(n[1], torch.Tensor)\n",
    "            else (n[0], n[1]) for n in self.recommendation\n",
    "        ]\n",
    "\n",
    "        return (f'\\n'\n",
    "                f'user_id: {self.user_id}, item_id: {self.item_id}\\n'\n",
    "                f'yloss: {round(self.y_loss, 4)}, proximity_loss: {int(self.proximity_loss)}\\n'\n",
    "                f'Item {self.item_id} is in position {self.pos} now!!!\\n'\n",
    "                f'Found in iteration {self.iter_no[\"best\"], {self.budget_spent}} and the interacted items are {self.interactions[\"best\"]}\\n'\n",
    "                f'10-best recommended items {sorted_recommended_items}\\n')\n",
    "\n",
    "    def set_flags(self, do_objective, do_contraints):\n",
    "        self.satisfy_objective = do_objective\n",
    "        self.satisfy_contraints = do_contraints\n",
    "\n",
    "    def needs_update(self, loss):\n",
    "        if len(loss):\n",
    "            does_contraints = (not self.satisfy_contraints or self.y_loss > loss['yloss'])\n",
    "            does_objective = (not self.satisfy_objective or self.proximity_loss >= loss['proximity'])\n",
    "\n",
    "            if does_contraints and does_objective: return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def set_values(self, predictions, interacted_items, tot_interacted_items, loss, iter_no, k=10):\n",
    "\n",
    "        # get the ranking position of selected item in the list\n",
    "        rk_data = st.rankdata(-predictions, method='ordinal')\n",
    "        self.pos = rk_data[self.item_id]\n",
    "#         self.recommends = sorted(enumerate(predictions), key=lambda x: x[1], reverse=True)[:k]\n",
    "        accepted_preds = (rk_data <= k).nonzero()\n",
    "        self.recommends = sorted(\n",
    "            zip(predictions[accepted_preds], *accepted_preds), \n",
    "            key=lambda x: x[0], reverse=True)\n",
    "        self.iter_found = iter_no\n",
    "        self.y_loss = loss[0]\n",
    "        self.proximity_loss = loss[1]\n",
    "        self.interactions = interacted_items\n",
    "        self.complete_interactions = tot_interacted_items\n",
    "\n",
    "        self.solution_found = True\n",
    "\n",
    "    def update_values(self, predictions, ranking, interacted_items, loss, iter_no, residual_budget, k):\n",
    "        # self.pos <= ranking[self.item_id]\n",
    "        if ranking[self.item_id] > k:\n",
    "\n",
    "            if loss < self.loss['best']:\n",
    "                # get the ranking position of selected item in the list\n",
    "                # rk_data = st.rankdata(-predictions, method='ordinal')\n",
    "                self.pos = ranking[self.item_id]\n",
    "        #         self.recommends = sorted(enumerate(predictions), key=lambda x: x[1], reverse=True)[:k]\n",
    "                accepted_preds = (ranking <= k).nonzero()\n",
    "                self.recommendation = sorted(\n",
    "                    zip(predictions[accepted_preds], *accepted_preds),\n",
    "                    key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                self.iter_no['best'] = iter_no\n",
    "                self.budget_spent['best'] = self.available_budget - residual_budget\n",
    "                self.loss['best'] = loss\n",
    "                self.interactions['best'] = interacted_items\n",
    "\n",
    "                if not self.solution_found:\n",
    "                    self.iter_no['initial'] = iter_no\n",
    "                    self.budget_spent['initial'] = self.available_budget - residual_budget\n",
    "                    self.loss['initial'] = loss\n",
    "                    self.interactions['initial'] = interacted_items\n",
    "\n",
    "                self.cfs_dist = self.len_interactions - len(self.interactions['best'])\n",
    "\n",
    "                self.stats_per_cardinality[self.len_interactions - len(interacted_items) - 1] = max(\n",
    "                    self.available_budget - residual_budget, self.stats_per_cardinality[len(interacted_items) - 1])\n",
    "\n",
    "            self.solution_found = True\n",
    "\n",
    "        self.iter_no['total'] = iter_no\n",
    "        self.budget_spent['total'] = self.available_budget - residual_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self, target, original_input, top_k=10, weights=[1, 0, 0], total_CFs=1):\n",
    "        self.target_item = target\n",
    "        self.top_k = top_k\n",
    "        self.original_items = original_input\n",
    "        self.total_CFs = total_CFs\n",
    "        (self.proximity_weight, self.diversity_weight, self.regularization_weight) = weights\n",
    "\n",
    "    def _compute_yloss(self, target_score, kth_score):\n",
    "        yloss = 0.0\n",
    "        for i in range(self.total_CFs):\n",
    "            temp_loss = max(0, target_score / kth_score - 1.0)\n",
    "            # temp_loss = target_score / kth_score\n",
    "\n",
    "            yloss += temp_loss\n",
    "        return yloss / self.total_CFs\n",
    "\n",
    "    def _compute_dist(self, x_hat, x1):\n",
    "        \"\"\"Compute weighted distance between two vectors.\"\"\"\n",
    "    #     return sum(abs(x_hat - x1))\n",
    "#         diff = set(x1).difference(set(x_hat))\n",
    "        diff = np.setdiff1d(x1, x_hat)\n",
    "        return len(diff)\n",
    "\n",
    "    def _compute_proximity_loss(self, cfs):\n",
    "        proximity_loss = 0.0\n",
    "        for i in range(self.total_CFs):\n",
    "            proximity_loss += self._compute_dist(cfs, self.original_items)\n",
    "        return proximity_loss / np.multiply(len(self.original_items), self.total_CFs)\n",
    "\n",
    "    def _compute_diversity_loss(self):\n",
    "        proximity_loss = 0.0\n",
    "        return proximity_loss / self.total_CFs\n",
    "\n",
    "    def _compute_regularization_loss(self, x):\n",
    "        \"\"\"Adds a linear equality constraints to the loss functions - to ensure all levels of a categorical variable sums to one\"\"\"\n",
    "        regularization_loss = 0.0\n",
    "        for i in range(self.total_CFs):\n",
    "            pass\n",
    "#             for v in self.encoded_categorical_feature_indexes:\n",
    "#                 regularization_loss += torch.pow((torch.sum(self.cfs[i][v[0]:v[-1]+1]) - 1.0), 2)\n",
    "#             regularization_loss += max(0, x - 1.0)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def compute_loss(self, cfs, preds, ranking, total_CFs=1):\n",
    "        \"\"\"Computes the overall loss\"\"\"\n",
    "\n",
    "        yloss = self._compute_yloss(preds[self.target_item], preds[(ranking == self.top_k).nonzero()][0])\n",
    "        proximity_loss = self._compute_proximity_loss(cfs) if self.proximity_weight > 0 else 0.0\n",
    "        diversity_loss = self._compute_diversity_loss() if self.diversity_weight > 0 else 0.0\n",
    "        regularization_loss = self._compute_regularization_loss(yloss) if self.regularization_weight > 0 else 0.0\n",
    "\n",
    "        loss = yloss + (self.proximity_weight * proximity_loss) \\\n",
    "            - (self.diversity_weight * diversity_loss) \\\n",
    "            + (self.regularization_weight * regularization_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_cfs(dataset, model, excluded_item_pos, no_users=None, max_allowed_permutations=None, top_k=10, total_CFs=1):\n",
    "    num_users = no_users or max(dataset.users_ids) + 1\n",
    "    max_perms = max_allowed_permutations or dataset.max_sequence_length\n",
    "\n",
    "    best_tot_loss_data = []\n",
    "    best_yloss_data = []\n",
    "\n",
    "    for user_id in trange(1, num_users):  # dataset.num_users):\n",
    "\n",
    "        seq_size = len(dataset.sequences[dataset.user_ids==user_id])\n",
    "        _total_loss = [None] * seq_size\n",
    "        _yloss = [None] * seq_size\n",
    "\n",
    "        for j in range(seq_size):    \n",
    "            if all(v > 0 for v in dataset.sequences[dataset.user_ids==user_id][j]):    \n",
    "                items_interacted = dataset.sequences[dataset.user_ids==user_id][j]\n",
    "                predictions = -model.predict(items_interacted)\n",
    "                predictions[items_interacted] = StaticVars.FLOAT_MAX\n",
    "\n",
    "                kth_item = predictions.argsort()[top_k - 1]\n",
    "                target_item = predictions.argsort()[min(top_k, int(excluded_item_pos)) - 1]\n",
    "\n",
    "                _total_loss[j] = InteractionsInfo(user_id, target_item)\n",
    "                _yloss[j] = InteractionsInfo(user_id, target_item, fobj=False)\n",
    "\n",
    "                loss = ComputeLoss(target_item, items_interacted, top_k)\n",
    "\n",
    "                counter = 1        \n",
    "\n",
    "                for l in range(len(items_interacted) - 1, max(0, len(items_interacted) - max_perms), -1):\n",
    "                    if _total_loss[j].solution_found: break\n",
    "\n",
    "                    # produce permutations of various interactions\n",
    "                    perm = combinations(items_interacted, l)\n",
    "\n",
    "                    for i in perm:\n",
    "                        # predict next top-k items about to be selected        \n",
    "                        preds = model.predict(i)\n",
    "                        \n",
    "                        # convert logits produced by model, i.e., the probability distribution before normalization, \n",
    "                        # by using softmax\n",
    "                        tensor = torch.from_numpy(preds).float()\n",
    "                        preds = F.softmax(tensor, dim=0)\n",
    "\n",
    "                        yloss = loss._compute_yloss(preds.numpy()[target_item], preds.numpy()[kth_item])\n",
    "                        proximity_loss = loss._compute_proximity_loss(np.asarray(i)[np.newaxis, :])\n",
    "                        \n",
    "                        # keep info about the best solution found depending on an objective function\n",
    "                        if _total_loss[j].needs_update(dict(yloss=yloss, proximity=proximity_loss)):                        \n",
    "                            _total_loss[j].set_values(\n",
    "                                preds, i, items_interacted, [yloss, proximity_loss], counter, top_k)\n",
    "                            \n",
    "#                         if _yloss[j].needs_update(dict(yloss=yloss, proximity=proximity_loss)):\n",
    "#                             _yloss[j].set_values(\n",
    "#                                 preds, i, items_interacted, [yloss, proximity_loss], counter, k)                 \n",
    "\n",
    "                        counter += 1 \n",
    "\n",
    "        best_tot_loss_data.append(_total_loss)\n",
    "        best_yloss_data.append(_yloss)\n",
    "        \n",
    "    return (best_tot_loss_data, best_yloss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count, RLock\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "def _retrieve_solutions(params):\n",
    "    user_id, d, m, sf, pos, init_budget, top_k, kwargs = params\n",
    "#     tqdm_text = \"#\" + \"{}\".format(pid).zfill(3)\n",
    "\n",
    "    _total_loss = []\n",
    "    seq = d.sequences[d.user_ids == user_id]\n",
    "    for j in range(min(1, len(seq))):  # seq_size):\n",
    "        if all(v > 0 for v in seq[j]):\n",
    "            items_interacted = seq[j].copy()\n",
    "            predictions = -m.predict(items_interacted)\n",
    "            predictions[items_interacted] = StaticVars.FLOAT_MAX\n",
    "\n",
    "            target_item = predictions.argsort()[min(top_k, int(pos)) - 1]\n",
    "\n",
    "            search_info = InteractionsInfo(user_id, target_item, items_interacted, init_budget)\n",
    "            loss = ComputeLoss(target_item, items_interacted, top_k)\n",
    "            strategy = sf(target_item, items_interacted, d.max_sequence_length, init_budget, m, **kwargs)\n",
    "\n",
    "            counter = 1\n",
    "            budget = strategy.get_init_budget()\n",
    "            while budget > 0:\n",
    "                perm, curr_budget = strategy.next_comb(reverse=search_info.solution_found)\n",
    "\n",
    "                if perm is None: break  # there is no need to continue searching\n",
    "\n",
    "                # predict next top-k items about to be selected\n",
    "                preds = m.predict(perm)\n",
    "                preds[perm] = -StaticVars.FLOAT_MAX\n",
    "                # already taken care in strategy func, so do not count. \n",
    "                # We exec model again to retrieve useful info to store\n",
    "#                 budget -= 1  # used Query\n",
    "\n",
    "                # normalize logits produced by model, i.e., the probability distribution before normalization, \n",
    "                # by using softmax\n",
    "#                             tensor = torch.from_numpy(preds).float()\n",
    "# #                             tensor = F.softmax(tensor, dim=0)\n",
    "#                             print('after', tensor, F.softmax(tensor, dim=-1), torch.max(tensor))\n",
    "\n",
    "                rk_data = st.rankdata(-preds, method='ordinal')\n",
    "                computed_loss = loss.compute_loss(perm, preds, rk_data)\n",
    "#                             print('stats', user_id, computed_loss, len(perm), rk_data[target_item])\n",
    "\n",
    "                # keep info about the best solution found depending on an objective function\n",
    "                search_info.update_values(\n",
    "                    preds, rk_data, perm, computed_loss, counter, curr_budget, top_k)\n",
    "\n",
    "                if hasattr(strategy, 'set_score'):\n",
    "                    reverse_search = strategy.set_score(\n",
    "                        len(items_interacted) - len(perm) - 1,\n",
    "                        preds[target_item],\n",
    "                        preds[(rk_data == top_k).nonzero()][0]\n",
    "                    )\n",
    "\n",
    "                    if reverse_search:\n",
    "                        _total_loss[j].solution_found = False\n",
    "#                                     print('Forward Search applied!!!', len(items_interacted) - len(perm) - 1)\n",
    "\n",
    "                strategy.reset_costs()\n",
    "                counter += 1\n",
    "        \n",
    "                budget = curr_budget\n",
    "\n",
    "            _total_loss.append(search_info)\n",
    "\n",
    "    return _total_loss\n",
    "\n",
    "\n",
    "def _find_cfs(dataset, model, strategy_func, target_item_pos, no_users=None, init_budget=1000,\n",
    "              max_allowed_permutations=None, top_k=10, total_CFs=1, num_processes=10, **kwargs):\n",
    "\n",
    "    print(f'The backend used is: {strategy_func.class_name}')\n",
    "\n",
    "    num_users = no_users or max(dataset.users_ids) + 1\n",
    "    best_tot_loss_data = dict.fromkeys(target_item_pos)\n",
    "\n",
    "    with tqdm(total=len(target_item_pos), desc='target position loop') as pbar:\n",
    "        for pos in target_item_pos:\n",
    "            pbar.update(10)\n",
    "\n",
    "            best_tot_loss_data[pos] = []\n",
    "            for user_id in trange(1, num_users + 1, desc='users loop', leave=False):  # dataset.num_users):\n",
    "    #                 best_tot_loss_data[pos].append(_total_loss)\n",
    "                best_tot_loss_data[pos].extend(_retrieve_solutions((\n",
    "                    user_id, dataset, model, strategy_func, pos, init_budget, top_k, kwargs)))\n",
    "\n",
    "\n",
    "#         pool = Pool(processes=min(num_processes, cpu_count() - 1, 4), initargs=(RLock(),), initializer=tqdm.set_lock)\n",
    "#         with Pool(processes=min(num_processes, cpu_count() - 1, 2), initializer=init, initargs=(l,)) as pool:\n",
    "    #         jobs = [pool.apply_async(_retrieve_solutions, args=((n,dataset,model,strategy_func,pos,init_budget,top_k,kwargs),)) \n",
    "    #             for n in range(1, num_users + 1)]\n",
    "#             jobs = list(pool.imap_unordered(_retrieve_solutions, zip(\n",
    "#                 range(1, num_users + 1), repeat(dataset), repeat(model), repeat(strategy_func),\n",
    "#                             repeat(pos), repeat(init_budget), repeat(top_k), repeat(kwargs)\n",
    "#                         )))\n",
    "#             best_tot_loss_data[pos] = [jobs[i].get() for i in trange(len(jobs))]\n",
    "\n",
    "#         with Pool(processes=min(num_processes, cpu_count() - 1, 2)) as p:\n",
    "# #             best_tot_loss_data[pos].append(list(tqdm(p.imap_unordered(\n",
    "# #                 _retrieve_solutions, zip(\n",
    "# #                     range(1, num_users + 1), repeat(dataset), repeat(model), repeat(strategy_func),\n",
    "# #                     repeat(pos), repeat(init_budget), repeat(top_k), repeat(kwargs)\n",
    "# #                 )), total=num_users, leave=False)\n",
    "# #             ))\n",
    "#             list(tqdm(p.imap_unordered(\n",
    "#                 _retrieve_solutions, zip(\n",
    "#                     range(1, num_users + 1), repeat(dataset), repeat(model), repeat(strategy_func),\n",
    "#                     repeat(pos), repeat(init_budget), repeat(top_k), repeat(kwargs)\n",
    "#                 )), total=num_users))\n",
    "# #                 r = list(tqdm(p.imap(_foo, range(30)), total=30))\n",
    "\n",
    "    return best_tot_loss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def convert_res_to_lists(cfs, cnt, non_achieved_target, technique):\n",
    "    for key, values in cfs.items():\n",
    "        total_data = []\n",
    "        cnt[key].setdefault(technique, [])\n",
    "        cfs_no = 0\n",
    "\n",
    "#         for items in values:\n",
    "        for rec in values:\n",
    "            if rec is None: continue\n",
    "\n",
    "#                 if not rec.solution_found or rec.pos < 10:\n",
    "#                     non_achieved_target[key].append(rec.user_id)\n",
    "#                     continue\n",
    "\n",
    "            total_data.append([\n",
    "                len(rec.interactions['original']) - len(rec.interactions['initial']), rec.cfs_dist,\n",
    "                # for boxplot\n",
    "                rec.budget_spent['initial'], rec.budget_spent['best'],\n",
    "                rec.iter_no['initial'], rec.iter_no['best'],\n",
    "                rec.user_id, len(rec.interactions['original'])\n",
    "            ] + rec.stats_per_cardinality)\n",
    "\n",
    "            cfs_no = len(rec.interactions['original'])\n",
    "\n",
    "        cnt[key][technique].append(Counter(item[0] for item in total_data))\n",
    "        cnt[key][technique].append(Counter(item[1] for item in total_data))\n",
    "        cnt[key][technique].append([item[2] for item in total_data])\n",
    "        cnt[key][technique].append([item[3] for item in total_data])\n",
    "        cnt[key][technique].append([item[4] for item in total_data])\n",
    "        cnt[key][technique].append([item[5] for item in total_data])\n",
    "        cnt[key][technique].append([item[6] for item in total_data])\n",
    "        cnt[key][technique].append([item[7] for item in total_data])\n",
    "        cnt[key][technique].append([item[1] for item in total_data])\n",
    "\n",
    "        for i in range(cfs_no):\n",
    "            cnt[key][technique].append([item[8 + i] for item in total_data])\n",
    "\n",
    "    return cnt, non_achieved_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\" \n",
    "    Converts a tensor of n embeddings to an (n, n) tensor of similarities.\n",
    "    \"\"\"\n",
    "    dot = E @ E.t()\n",
    "    norm = torch.norm(E, 2, 1)\n",
    "    x = torch.div(dot, norm)\n",
    "    x = torch.div(x, torch.unsqueeze(norm, -1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "def embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\" \n",
    "    Converts a a tensor of n embeddings to an (n, n) tensor of similarities.\n",
    "    \"\"\"\n",
    "    similarities = [[cosine_similarity(a, b, dim=0) for a in E] for b in E]\n",
    "#     similarities = list(map(torch.cat, similarities))\n",
    "    similarities = list(map(lambda x: torch.stack(x, dim=-1), similarities))\n",
    "    return torch.stack(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "def compute_sim_matrix(dataset, metric='jaccard', adjusted=False):\n",
    "    # compute the item-item similarity matrix utilizing implicit feedback,\n",
    "    # i.e., whether interacted or not with an item\n",
    "\n",
    "    M = np.zeros((dataset.num_users, dataset.num_items), dtype=np.bool_)\n",
    "    for u in trange(1, dataset.num_users):\n",
    "        np.add.at(\n",
    "            M[u], dataset.item_ids[dataset.user_ids == u],\n",
    "            dataset.ratings[dataset.user_ids == u]\n",
    "        )\n",
    "\n",
    "    if adjusted:\n",
    "        M_u = M.mean(axis=1)\n",
    "        M = M - M_u[:, np.newaxis]\n",
    "\n",
    "    similarity_matrix = 1 - squareform(pdist(M.T, metric))\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def rank_interactions_to_excluded_item_per_user(cfs, sims_matrix):\n",
    "    non_solvable_cases = []\n",
    "    total_data = []\n",
    "\n",
    "    for items in cfs:\n",
    "        for rec in items:\n",
    "            if rec is None: continue\n",
    "\n",
    "            if not rec.solution_found:\n",
    "                non_solvable_cases.append(rec.user_id)\n",
    "                continue\n",
    "\n",
    "            items_rank = st.rankdata(sims_matrix[rec.item_id, rec.complete_interactions])\n",
    "            similarity_rank = len(rec.complete_interactions) - items_rank + 1\n",
    "            del_items_indices = np.where(np.isin(\n",
    "                rec.complete_interactions, \n",
    "                list(set(rec.complete_interactions).difference(set(rec.interactions)))\n",
    "            ))\n",
    "            total_data.extend(sorted(similarity_rank[del_items_indices].astype(int)[-1:]))\n",
    "\n",
    "    return (Counter(total_data), non_solvable_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple class stack that only allows pop and push operations\n",
    "class Stack:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.stack) < 1:\n",
    "            return None\n",
    "        return self.stack.pop()\n",
    "\n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "\n",
    "# And a queue that only has enqueue and dequeue operations\n",
    "class Queue:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.queue = []\n",
    "\n",
    "    def enqueue(self, item):\n",
    "        self.queue.append(item)\n",
    "\n",
    "    def dequeue(self):\n",
    "        if len(self.queue) < 1:\n",
    "            return None\n",
    "        return self.queue.pop(0)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def clear(self):\n",
    "        del self.queue[:]\n",
    "\n",
    "    def get(self, i):\n",
    "        return self.queue[i]\n",
    "    \n",
    "    def setter(self, i, v):\n",
    "        self.queue[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CFExplainability",
   "language": "python",
   "name": "cfexplainability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
